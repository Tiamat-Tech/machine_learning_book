\titlespacing*{\part}{0pt}{-20pt}{30pt} % to fix table and plot together on the first page (SEE RESTORE AT BOTTOM)
\titlespacing*{\chapter}{0pt}{-10pt}{30pt}

\part{Unsupervised Learning}
\label{part:unsupervised_learning}

\marginnote{From CS229 Spring 2021, Andrew Ng, Moses Charikar, Christopher R\'e \& Tengyu Ma, Stanford University.}

\vspace{1cm}
\inlinechapter{The $k$-means Clustering Algorithm}\label{cha:k_means}
In the clustering problem, we are given a training set $\{x^{(1)} ,\ldots,x^{(n)}\}$, and
want to group the data into a few cohesive ``clusters.'' Here, $x^{(i)} \in \mathbb R^d$
as usual; but no labels $y^{(i)}$ are given. So, this is an unsupervised learning
problem.
The k-means clustering algorithm is as follows:
\begin{enumerate}
    \item Initialize \textbf{cluster centroids} $\mu_1 ,\mu_2 ,\ldots,\mu_k \in \mathbb R^d$ randomly.
    \item Repeat until convergence:
    \begin{itemize}
        \item For every $i$, set:
        \begin{equation*}
            c^{(i)} := \argmin_j \lVert x^{(i)} - \mu-j \rVert^2
        \end{equation*}
        \item For each $j$, set:
        \begin{equation*}
            \mu_j := \frac{\sum^n_{i=1} \mathbb{1}\{c^{(i)} = j\}x^{(i)}}{\sum^n_{i=1} \mathbb{1}\{c^{(i)} = j\}}
        \end{equation*}
    \end{itemize}
\end{enumerate}

In the algorithm above, $k$ (a parameter of the algorithm) is the number
of clusters we want to find; and the cluster centroids $\mu_j$ represent our current
guesses for the positions of the centers of the clusters. To initialize the cluster
centroids (in step 1 of the algorithm above), we could choose $k$ training
examples randomly, and set the cluster centroids to be equal to the values of
these $k$ examples. (Other initialization methods are also possible.)

The inner-loop of the algorithm repeatedly carries out two steps: (i)
``Assigning'' each training example $x^{(i)}$ to the closest cluster centroid $\mu_j$ , and
(ii) Moving each cluster centroid $\mu_j$ to the mean of the points assigned to it.
Figure 1 shows an illustration of running k-means.
% TODO: k-means figure.
% Figure 1: K-means algorithm. Training examples are shown as dots, and
% cluster centroids are shown as crosses. (a) Original dataset. (b) Random initial
% cluster centroids (in this instance, not chosen to be equal to two training
% examples). (c-f) Illustration of running two iterations of $k$-means. In each
% iteration, we assign each training example to the closest cluster centroid
% (shown by ``painting'' the training examples the same color as the cluster
% centroid to which is assigned); then we move each cluster centroid to the
% mean of the points assigned to it. (Best viewed in color.) Images courtesy
% Michael Jordan.

Is the $k$-means algorithm guaranteed to converge? Yes it is, in a certain
sense. In particular, let us define the distortion function to be:
\begin{equation*}
    J(c,\mu) = \sum^n_{i=1} \lVert x^{(i)} - \mu_{c^{(i)}} \rVert^2
\end{equation*}
Thus, $J$ measures the sum of squared distances between each training
example $x^{(i)}$ and the cluster centroid $\mu_{c^{(i)}}$ to which it has been assigned. It can
be shown that $k$-means is exactly coordinate descent on $J$. Specifically, the
inner-loop of $k$-means repeatedly minimizes $J$ with respect to $c$ while holding
$\mu$ fixed, and then minimizes $J$ with respect to $\mu$ while holding $c$ fixed. Thus,
$J$ must monotonically decrease, and the value of $J$ must converge. (Usually,
this implies that $c$ and $\mu$ will converge too. In theory, it is possible for
$k$-means to oscillate between a few different clusterings---i.e., a few different
values for $c$ and/or $\mu$---that have exactly the same value of $J$, but this almost
never happens in practice.)

The distortion function $J$ is a non-convex function, and so coordinate
descent on $J$ is not guaranteed to converge to the global minimum. In other
words, $k$-means can be susceptible to local optima. Very often $k$-means will
work fine and come up with very good clusterings despite this. But if you
are worried about getting stuck in bad local minima, one common thing to
do is run $k$-means many times (using different random initial values for the
cluster centroids $\mu_j$ ). Then, out of all the different clusterings found, pick
the one that gives the lowest distortion $J(c,\mu)$.



\vspace{1cm}
\begin{fullwidth}
\inlinechapter{Mixtures of Gaussians and the EM Algorithm}\label{cha:gmm}
\end{fullwidth}
In this chapter, we discuss the EM (Expectation-Maximization) algorithm % DIFF: "chapter"
for density estimation.

Suppose that we are given a training set $\{x^{(1)},\ldots,x^{(n)}\}$ as usual. Since
we are in the unsupervised learning setting, these points do not come with
any labels.
% DIFF: no paragraph break.
We wish to model the data by specifying a joint distribution $p(x^{(i)} ,z^{(i)} ) =
p(x^{(i)} \mid z^{(i)} )p(z^{(i)} )$. Here, $z^{(i)} \sim \operatorname{Multinomial}(\phi)$ (where $\phi_j \ge 0,
\sum^k_{j=1} \phi_j = 1$,
and the parameter $\phi_j$ gives $p(z^{(i)} = j)$), and $x^{(i)} \mid z^{(i)} = j \sim \mathcal N(\mu_j ,\Sigma_j )$. We
let $k$ denote the number of values that the $z^{(i)}$ 's can take on. Thus, our
model posits that each $x^{(i)}$ was generated by randomly choosing $z^{(i)}$ from
$\{1,\ldots,k\}$, and then $x^{(i)}$ was drawn from one of $k$ Gaussians depending on
$z^{(i)}$ . This is called the \textbf{mixture of Gaussians} model. Also, note that the
$z^{(i)}$'s are \textbf{latent} random variables, meaning that they're hidden/unobserved.
This is what will make our estimation problem difficult.

The parameters of our model are thus $\phi$, $\mu$ and $\Sigma$. To estimate them, we
can write down the likelihood of our data:
\begin{align*}
    \ell(\phi,\mu,\Sigma) &= \sum^n_{i=1} \log p(x^{(i)} ;\phi,\mu,\Sigma)\\
        &= \sum^n_{i=1} \log \sum^k_{z^{(i)} = 1} p(x^{(i)} \mid z^{(i)} ;\mu,\Sigma)p(z^{(i)} ;\phi)
\end{align*}
However, if we set to zero the derivatives of this formula with respect to
the parameters and try to solve, we'll find that it is not possible to find the
maximum likelihood estimates of the parameters in closed form. (Try this
yourself at home.)

The random variables $z^{(i)}$ indicate which of the $k$ Gaussians each $x^{(i)}$
had come from. Note that if we knew what the $z^{(i)}$'s were, the maximum
likelihood problem would have been easy. Specifically, we could then write
down the likelihood as:
\begin{equation*}
    \ell(\phi,\mu,\Sigma) = \sum_{i=1}^n \log p(x^{(i)} \mid z^{(i)} ;\mu,\Sigma) + \log p(z^{(i)} ;\phi)
\end{equation*}
Maximizing this with respect to $\phi$, $\mu$ and $\Sigma$ gives the parameters:
\begin{align*}
    \phi_j &= \frac{1}{n} \sum_{i=1}^n \mathbb{1}\{z^{(i)} = j\}\\
    \mu_j &= \frac{\sum^n_{i=1} \mathbb{1}\{z^{(i)} = j\}x^{(i)}}{\sum^n_{i=1} \mathbb{1}\{z^{(i)} = j\}}\\
    \Sigma_j &= \frac{\sum^n_{i=1} \mathbb{1}\{z^{(i)} = j\}(x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^\top}{\sum^n_{i=1} \mathbb{1}\{z^{(i)} = j\}}
\end{align*}

Indeed, we see that if the $z^{(i)}$ 's were known, then maximum likelihood
estimation becomes nearly identical to what we had when estimating the
parameters of the Gaussian discriminant analysis model, except that here
the $z^{(i)}$'s playing the role of the class labels.\footnote{
There are other minor differences in the formulas here from what we'd obtained in
PS1 with Gaussian discriminant analysis, first because we've generalized the $z^{(i)}$'s to be
multinomial rather than Bernoulli, and second because here we are using a different $\Sigma_j$
for each Gaussian.}

However, in our density estimation problem, the $z^{(i)}$ 's are not known.
What can we do?
% DIFF: no paragraph break
The EM algorithm is an iterative algorithm that has two main steps.
Applied to our problem, in the E-step, it tries to ``guess'' the values of the
$z^{(i)}$ 's. In the M-step, it updates the parameters of our model based on our
guesses. Since in the M-step we are pretending that the guesses in the first
part were correct, the maximization becomes easy. Here's the algorithm:
\begin{itemize}
    \item Repeat until convergence:
    \begin{itemize}
        \item (E-step) For each $i,j$, set:
        \begin{equation*}
            w^{(i)}_j := p(z^{(i)} = j \mid x^{(i)} ;\phi,\mu,\Sigma)
        \end{equation*}
        \item (M-step) Update the parameters:
        \begin{align*}
            \phi_j &= \frac{1}{n} \sum_{i=1}^n w^{(i)}_j \\
            \mu_j &= \frac{\sum^n_{i=1} w^{(i)}_j x^{(i)}}{\sum^n_{i=1} w^{(i)}_j }\\
            \Sigma_j &= \frac{\sum^n_{i=1} w^{(i)}_j (x^{(i)} - \mu_j)(x^{(i)} - \mu_j)^\top}{\sum^n_{i=1} w^{(i)}_j }            
        \end{align*}

    \end{itemize}
\end{itemize}

In the E-step, we calculate the posterior probability of our parameters
the $z^{(i)}$ 's, given the $x^{(i)}$ and using the current setting of our parameters. I.e.,
using Bayes rule, we obtain:
\begin{equation*}
    p(z^{(i)} = j \mid x^{(i)} ;\phi,\mu,\Sigma) = \frac{p(x^{(i)} \mid z^{(i)} = j;\mu,\Sigma)p(z^{(i)} = j;\phi)}{\sum^k_{l=1} p(x^{(i)} \mid z^{(i)} = l;\mu,\Sigma)p(z^{(i)} = l;\phi)}
\end{equation*}
Here, $p(x^{(i)} \mid z^{(i)} = j;\mu,\Sigma)$ is given by evaluating the density of a Gaussian
with mean $\mu_j$ and covariance $\Sigma_j$ at $x^{(i)}$ ; $p(z^{(i)} = j;\phi)$ is given by $\phi_j$ , and so
on. The values $w^{(i)}_j$ calculated in the E-step represent our ``soft'' guesses\footnote{
The term ``soft'' refers to our guesses being probabilities and taking values in $[0,1]$; in
contrast, a ``hard'' guess is one that represents a single best guess (such as taking values
in $\{0,1\}$ or $\{1,\ldots,k\}$).} for the values of $z^{(i)}$.

Also, you should contrast the updates in the M-step with the formulas we
had when the $z^{(i)}$'s were known exactly. They are identical, except that instead
of the indicator functions ``$\mathbb{1}\{z^{(i)} = j\}$'' indicating from which Gaussian
each datapoint had come, we now instead have the $w^{(i)}_j$'s.

The EM-algorithm is also reminiscent of the K-means clustering algorithm,
except that instead of the ``hard'' cluster assignments $c(i)$, we instead
have the ``soft'' assignments $w^{(i)}_j$. Similar to K-means, it is also susceptible
to local optima, so reinitializing at several different initial parameters may
be a good idea.

It's clear that the EM algorithm has a very natural interpretation of
repeatedly trying to guess the unknown $z^{(i)}$'s; but how did it come about,
and can we make any guarantees about it, such as regarding its convergence?
In the next set of notes, we will describe a more general view of EM, one
that will allow us to easily apply it to other estimation problems in which
there are also latent variables, and which will allow us to give a convergence
guarantee.
