\titlespacing*{\part}{0pt}{-20pt}{30pt} % to fix table and plot together on the first page (SEE RESTORE AT BOTTOM)
\titlespacing*{\chapter}{0pt}{-10pt}{30pt}

\begin{fullwidth}
\part{Regularization and Model Selection}
\label{part:regularization}
\end{fullwidth}

\marginnote{From CS229 Spring 2021, Andrew Ng, Moses Charikar, Christopher R\'e \& Yoann Le Calonnec, Stanford University.}

Suppose we are trying select among several different models for a learning
problem. For instance, we might be using a polynomial regression model
$h_\theta (x) = g(\theta_0 + \theta_1 x + \theta_2 x^2 + \cdots + \theta_k x^k)$, and wish to decide if $k$ should be
$0, 1, \ldots,$ or $10$. How can we automatically select a model that represents a
good tradeoff between the twin evils of bias and variance?\footnote{
Given that we said in the previous set of notes that bias and variance are two very
different beasts, some readers may be wondering if we should be calling them ``twin'' evils
here. Perhaps it'd be better to think of them as non-identical twins. The phrase ``the
fraternal twin evils of bias and variance'' doesn't have the same ring to it, though.} Alternatively,
suppose we want to automatically choose the bandwidth parameter $\tau$ for
locally weighted regression, or the parameter $C$ for our $\ell_1$-regularized SVM.
How can we do that?

For the sake of concreteness, in these notes we assume we have some
finite set of models $\mathcal M = \{M_1 ,\ldots ,M_d\}$ that we're trying to select among.
For instance, in our first example above, the model $M_i$ would be an $i$-th
order polynomial regression model. (The generalization to infinite $\mathcal M$ is not
hard.\footnote{
If we are trying to choose from an infinite set of models, say corresponding to the
possible values of the bandwidth $\tau \in \mathbb R^+$ , we may discretize $\tau$ and consider only a finite
number of possible values for it. More generally, most of the algorithms described here
can all be viewed as performing optimization search in the space of models, and we can
perform this search over infinite model classes as well.}) Alternatively, if we are trying to decide between using an SVM, a
neural network or logistic regression, then $\mathcal M$ may contain these models.


\vspace{1cm}
\begin{fullwidth}
\inlinechapter{Cross validation}\label{sec:cross_validation}
\end{fullwidth}
Lets suppose we are, as usual, given a training set $S$. Given what we know
about empirical risk minimization, here's what might initially seem like a
algorithm, resulting from using empirical risk minimization for model selection:
\begin{enumerate}
    \item Train each model $M_i$ on $S$, to get some hypothesis $h_i$.
    \item Pick the hypotheses with the smallest training error.
\end{enumerate}

This algorithm does \textit{not} work. Consider choosing the order of a polynomial.
The higher the order of the polynomial, the better it will fit the
training set $S$, and thus the lower the training error. Hence, this method will
always select a high-variance, high-degree polynomial model, which we saw
previously is often poor choice.

Here's an algorithm that works better. In \textbf{hold-out cross validation}
(also called \textbf{simple cross validation}), we do the following:
\begin{enumerate}
    \item Randomly split $S$ into $S_\text{train}$ (say, $70\%$ of the data) and $S_\text{cv}$ (the remaining $30\%$). Here, $S_\text{cv}$ is called the hold-out cross validation set.
    \item Train each model $M_i$ on $S_\text{train}$ only, to get some hypothesis $h_i$ .
    \item Select and output the hypothesis $h_i$ that had the smallest error $\hat{\varepsilon}_{S_\text{cv}}(h_i)$ on the hold out cross validation set. (Recall,  $\hat{\varepsilon}_{S_\text{cv}}(h)$ denotes the empirical error of $h$ on the set of examples in $S_\text{cv}$.)
\end{enumerate}

By testing on a set of examples $S_\text{cv}$ that the models were not trained on,
we obtain a better estimate of each hypothesis $h_i$'s true generalization error,
and can then pick the one with the smallest estimated generalization error.
Usually, somewhere between $1/4 - 1/3$ of the data is used in the hold out
cross validation set, and $30\%$ is a typical choice.

Optionally, step 3 in the algorithm may also be replaced with selecting
the model $M_i$ according to $\argmin_i \hat{\varepsilon}_{S_\text{cv}}(h_i)$, and then retraining $M_i$ on the
entire training set $S$. (This is often a good idea, with one exception being
learning algorithms that are be very sensitive to perturbations of the initial
conditions and/or data. For these methods, $M_i$ doing well on $S_\text{train}$ does not
necessarily mean it will also do well on $S_\text{cv}$, and it might be better to forgo
this retraining step.)

The disadvantage of using hold out cross validation is that it ``wastes''
about $30\%$ of the data. Even if we were to take the optional step of retraining
the model on the entire training set, it's still as if we're trying to find a good
model for a learning problem in which we had 0.7$m$ training examples, rather
than $n$ training examples, since we're testing models that were trained on
only 0.7$m$ examples each time. While this is fine if data is abundant and/or
cheap, in learning problems in which data is scarce (consider a problem with
$m = 20$, say), we'd like to do something better.

Here is a method, called $k$\textbf{-fold cross validation}, that holds out less
data each time:
\begin{enumerate}
    \item Randomly split $S$ into $k$ disjoint subsets of $m/k$ training examples each. Lets call these subsets $S_1,\ldots,S_k$.
    \item For each model $M_i$, we evaluate it as follows:
    \begin{itemize}
        \item For $j=1,\ldots,k$:
        \begin{itemize}
            \item Train the model $M_i$ on $S_1 \cup \cdots \cup S_{j-1} \cup S_{j+1} \cup \cdots S_k$ (i.e., train on all the data except $S_j$) to get some hypothesis $h_{ij}$.
            \item Test the hypothesis $h_{ij}$ on $S_j$, to get $\hat{\varepsilon}_{S_j}(h_{ij})$.
        \end{itemize}
        \item The estimated generalization error of model $M_i$ is then calculated as the average of the $\hat{\varepsilon}_{S_j} (h_{ij})$'s (averaged over $j$).
    \end{itemize}
    \item Pick the model $M_i$ with the lowest estimated generalization error, and retrain that model on the entire training set $S$. The resulting hypothesis is then output as our final answer.
\end{enumerate}

A typical choice for the number of folds to use here would be $k = 10$.
While the fraction of data held out each time is now $1/k$---much smaller
than before---this procedure may also be more computationally expensive
than hold-out cross validation, since we now need train to each model $k$
times.

While $k = 10$ is a commonly used choice, in problems in which data is
really scarce, sometimes we will use the extreme choice of $k = m$ in order
to leave out as little data as possible each time. In this setting, we would
repeatedly train on all but one of the training examples in $S$, and test on that
held-out example. The resulting $m = k$ errors are then averaged together to
obtain our estimate of the generalization error of a model. This method has
its own name; since we're holding out one training example at a time, this
method is called \textbf{leave-one-out cross validation}.

Finally, even though we have described the different versions of cross
validation as methods for selecting a model, they can also be used more simply to
evaluate a \textit{single} model or algorithm. For example, if you have implemented
some learning algorithm and want to estimate how well it performs for your
application (or if you have invented a novel learning algorithm and want to
report in a technical paper how well it performs on various test sets), cross
validation would give a reasonable way of doing so.


\vspace{1cm}
\inlinechapter{Feature Selection}
One special and important case of model selection is called feature selection.
To motivate this, imagine that you have a supervised learning problem where
the number of features $d$ is very large (perhaps $d \gg n$), but you suspect that
there is only a small number of features that are ``relevant'' to the learning
task. Even if you use the a simple linear classifier (such as the perceptron)
over the $d$ input features, the VC dimension of your hypothesis class would
still be $O(n)$, and thus overfitting would be a potential problem unless the
training set is fairly large.

In such a setting, you can apply a feature selection algorithm to reduce the
number of features. Given $d$ features, there are $2^d$ possible feature subsets
(since each of the $d$ features can either be included or excluded from the
subset), and thus feature selection can be posed as a model selection problem
over $2^d$ possible models. For large values of $d$, it's usually too expensive to
explicitly enumerate over and compare all $2^d$ models, and so typically some
heuristic search procedure is used to find a good feature subset. The following
search procedure is called \textbf{forward search}:

\begin{algorithm}[ht]
    \caption{Forward search.}
    \label{alg:forward_search}
    \begin{algorithmic}
    \State Initialize $\mathcal{F} = \emptyset$.
    \Repeat
        \For{$i=1,\ldots,d$}
            \If{$i \not\in \mathcal{F}$}
                \State $\mathcal{F}_i = \mathcal{F} \cup \{i\}$
                \State Use some version of cross validation to evaluate features $\mathcal{F}_i$.
                \State (i.e., train your learning algorithm using only the features in $\mathcal{F}_i$, and estimate its generalization error.)
            \EndIf
        \EndFor
        \State Set $\mathcal{F}$ to be the best feature subset found in the previous step. % DIFF.
    \Until{convergence}
    \State Select and output the best feature subset that was evaluated during the entire search procedure.
    \end{algorithmic}
\end{algorithm}

The outer loop of the algorithm can be terminated either when $\mathcal F =
\{1,\ldots ,d\}$ is the set of all features, or when $|\mathcal F|$ exceeds some pre-set
threshold (corresponding to the maximum number of features that you want the
algorithm to consider using).

This algorithm described above one instantiation of \textbf{wrapper model
feature selection}, since it is a procedure that ``wraps'' around your learning
algorithm, and repeatedly makes calls to the learning algorithm to evaluate
how well it does using different feature subsets. Aside from forward search,
other search procedures can also be used. For example, \textbf{backward search}
starts off with $\mathcal F = \{1,\ldots ,d\}$ as the set of all features, and repeatedly deletes
features one at a time (evaluating single-feature deletions in a similar manner
to how forward search evaluates single-feature additions) until $\mathcal F = \emptyset$.

Wrapper feature selection algorithms often work quite well, but can be
computationally expensive given how that they need to make many calls to
the learning algorithm. Indeed, complete forward search (terminating when
$\mathcal F = \{1,\ldots ,d\}$) would take about $O(n^2)$ calls to the learning algorithm.

\textbf{Filter feature selection} methods give heuristic, but computationally
much cheaper, ways of choosing a feature subset. The idea here is to compute
some simple score $S(i)$ that measures how informative each feature $x_i$ is about
the class labels $y$. Then, we simply pick the $k$ features with the largest scores
$S(i)$.

One possible choice of the score would be define $S(i)$ to be (the absolute
value of) the correlation between $x_i$ and $y$, as measured on the training data.
This would result in our choosing the features that are the most strongly
correlated with the class labels. In practice, it is more common (particularly
for discrete-valued features $x_i$ ) to choose $S(i)$ to be the mutual information
$\operatorname{MI}(x_i ,y)$ between $x_i$ and $y$:
\begin{equation}
    \operatorname{MI}(x_i ,y) = \sum_{x_i \in \{0,1\}} \sum_{y \in \{0,1\}} p(x_i ,y)\log \frac{p(x_i ,y)}{p(x_i )p(y)}
\end{equation}
(The equation above assumes that $x_i$ and $y$ are binary-valued; more generally
the summations would be over the domains of the variables.) The
probabilities above $p(x_i ,y)$, $p(x_i )$ and $p(y)$ can all be estimated according to their
empirical distributions on the training set.

To gain intuition about what this score does, note that the mutual
information can also be expressed as a Kullback-Leibler (KL) divergence:
\begin{equation}
    \operatorname{MI}(x_i ,y) = \operatorname{KL}(p(x_i,y) \mid\mid p(x_i)p(y))
\end{equation}
You'll get to play more with KL-divergence in the problem sets, but
informally, this gives a measure of how different the probability distributions
$p(x_i ,y)$ and $p(x_i )p(y)$ are. If $x_i$ and $y$ are independent random variables,
then we would have $p(x_i ,y) = p(x_i )p(y)$, and the KL-divergence between the
two distributions will be zero. This is consistent with the idea if $x_i$ and $y$ are
independent, then $x_i$ is clearly very ``non-informative'' about $y$, and thus the
score $S(i)$ should be small. Conversely, if $x_i$ is very ``informative'' about $y$,
then their mutual information $\operatorname{MI}(x_i ,y)$ would be large.

One final detail: Now that you've ranked the features according to their
scores $S(i)$, how do you decide how many features $k$ to choose? Well, one
standard way to do so is to use cross validation to select among the possible
values of $k$. For example, when applying naive Bayes to text classification---
a problem where $d$, the vocabulary size, is usually very large---using this
method to select a feature subset often results in increased classifier accuracy.


\vspace{1cm}
\inlinechapter{Bayesian statistics and regularization}
In this section, we will talk about one more tool in our arsenal for our battle
against overfitting.

At the beginning of the quarter, we talked about parameter fitting using
maximum likelihood estimation (MLE), and chose our parameters according
to
\begin{equation}
    \theta_\text{MLE} = \argmax_\theta \prod_{i=1}^n p(y^{(i)} \mid x^{(i)}; \theta).
\end{equation}
Throughout our subsequent discussions, we viewed $\theta$ as an unknown
parameter of the world. This view of the $\theta$ as being \textit{constant-valued but unknown} is
taken in \textbf{frequentist} statistics. In the frequentist this view of the world, $\theta$
is not random---it just happens to be unknown---and it's our job to come up
with statistical procedures (such as maximum likelihood) to try to estimate
this parameter.

An alternative way to approach our parameter estimation problems is
to take the \textbf{Bayesian} view of the world, and think of $\theta$ as being a
\textit{random variable} whose value is unknown. In this approach, we would specify
a prior distribution $p(\theta)$ on $\theta$ that expresses our ``prior beliefs'' about the
parameters. Given a training set $S = \{(x^{(i)}, y^{(i)})\}^n_{i=1}$, when we are asked to
make a prediction on a new value of $x$, we can then compute the posterior
distribution on the parameters:
\begin{align}\label{eq:posterior}
    p(\theta \mid S) &= \frac{p(S \mid \theta)p(\theta)}{p(S)}\\
        &= \frac{\left(\prod_{i=1}^n p(y^{(i)} \mid x^{(i)}, \theta) \right) p(\theta)}{\int_\theta \left(\prod_{i=1}^n p(y^{(i)} \mid x^{(i)}, \theta) p(\theta) \right) d\theta}
\end{align}
In the equation above, $p(y^{(i)} \mid x^{(i)}, \theta)$ comes from whatever model you're using
for your learning problem. For example, if you are using Bayesian logistic
regression, then you might choose $p(y^{(i)} \mid x^{(i)},\theta) = h_\theta (x^{(i)})^{y^{(i)}} (1-h_\theta (x^{(i)}))^{(1-y^{(i)}}) $, where $h_\theta(x^{(i)}) = 1/(1 + \exp(-\theta^\top x^{(i)}))$.\footnote{
Since we are now viewing $\theta$ as a random variable, it is okay to condition on its value, % TYPO: "it" -> "its"
and write ``$p(y|x,\theta)$'' instead of ``$p(y|x;\theta)$.''}

When we are given a new test example $x$ and asked to make a prediction % DIFF: it -> a
on it, we can compute our posterior distribution on the class label using the
posterior distribution on $\theta$:
\begin{equation}\label{eq:conditional}
    p(y \mid x,S) = \int_\theta p(y \mid x,\theta)p(\theta \mid S)d\theta
\end{equation}
In the equation above, $p(\theta \mid S)$ comes from \cref{eq:posterior}. Thus, for example,
if the goal is to the predict the expected value of $y$ given $x$, then we would
output:\footnote{The integral below would be replaced by a summation if $y$ is discrete-valued.}
\begin{equation}
    \mathbb E[y \mid x,S] = \int_y y p(y \mid x,S)dy
\end{equation}

The procedure that we've outlined here can be thought of as doing ``fully
Bayesian'' prediction, where our prediction is computed by taking an average
with respect to the posterior $p(\theta \mid S)$ over $\theta$. Unfortunately, in general it is
computationally very difficult to compute this posterior distribution. This is
because it requires taking integrals over the (usually high-dimensional) $\theta$ as
in \cref{eq:posterior}, and this typically cannot be done in closed-form.

Thus, in practice we will instead approximate the posterior distribution
for $\theta$. One common approximation is to replace our posterior distribution for
$\theta$ (as in \cref{eq:conditional}) with a single point estimate. The \textbf{MAP} (\textbf{maximum
a posteriori}) estimate for $\theta$ is given by:
\begin{equation}\label{eq:map}
    \theta_\text{MAP} = \argmax_\theta \prod^n_{i=1} p(y^{(i)} \mid x^{(i)}, \theta)p(\theta)    
\end{equation}
Note that this is the same formulas as for the MLE (maximum likelihood)
estimate for $\theta$, except for the prior $p(\theta)$ term at the end.

In practical applications, a common choice for the prior $p(\theta)$ is to assume
that $\theta \sim \mathcal N(0, \tau^2 I)$. Using this choice of prior, the fitted parameters $\theta$ MAP will
have smaller norm than that selected by maximum likelihood. In practice,
this causes the Bayesian MAP estimate to be less susceptible to overfitting
than the ML estimate of the parameters. For example, Bayesian logistic
regression turns out to be an effective algorithm for text classification, even
though in text classification we usually have $d \gg n$.




\vspace{1cm}
\inlinechapter{Some calculations from bias variance}
\marginnote{From CS229 Fall 2020, Christopher R\'e, Stanford University.}
This section contains a reprise of the eigenvalue arguments to understand how
variance is reduced by regularization. We also describe different ways
regularization can occur including from the algorithm or initialization. This note
contains some additional calculations from the lecture and Piazza, just so that
we have typeset versions of them. They contain no new information over the
lecture, but they do supplement the previous sections. % DIFF: "notes" to "sections"

Recall we have a design matrix $X \in \mathbb R^{n \times d}$ and labels $y \in \mathbb R^n$. We are
interested in the underdetermined case $n < d$ so that $\operatorname{rank}(X) \le n < d$. We consider
the following optimization problem for least squares with a regularization
parameter $\lambda \ge 0$:
\begin{equation}
    \ell(\theta;\lambda) = \min_{\theta \in \mathbb R^d} \frac{1}{2} \lVert X \theta - y\rVert^2 + \frac{\lambda}{2} \lVert \theta\rVert^2
\end{equation}

\paragraph{Normal equations} Computing derivatives as we did for the normal
equations, we see that:
\begin{equation}
    \nabla_\theta \ell(\theta; \lambda) = X^\top(X\theta - y) + \lambda\theta = (X^\top X + \lambda I)\theta - X^\top y
\end{equation}
By setting $\nabla_\theta \ell(\theta,\lambda) = 0$ we can solve for the $\hat{\theta}$
that minimizes the above problem. Explicitly, we have:
\begin{equation}\label{eq:theta_hat}
    \hat{\theta} = ( X^\top X + \lambda I)^{-1} X^\top y
\end{equation}
To see that the inverse in \cref{eq:theta_hat} exists, we observe that $X^\top X$ is a symmetric,
real $d \times d$ matrix so it has $d$ eigenvalues (some may be $0$). Moreover, it is positive
semidefinite, and we capture this by writing $\operatorname{eig}(X^\top X) = \{\sigma^2_1 ,\ldots, \sigma^2_d\}$.
Now, inspired by the regularized problem, we examine:
\begin{equation}
    \operatorname{eig}(X^\top X + \lambda I) = \left\{ \sigma^2_1 + \lambda,\ldots,\sigma^2_d + \lambda\right\}
\end{equation}
Since $\sigma^2_i \ge 0$ for all $i \in [d]$, if we set $\lambda > 0$ then $X^\top X +\lambda I$ is full rank, and the
inverse of $(X^\top X + \lambda I)$ exists. In turn, this means there is a unique such $\hat{\theta}$.

\paragraph{Variance} Recall that in bias-variance, we are concerned with the variance of
$\hat{\theta}$ as we sample the training set. We want to argue that as the regularization
parameter $\lambda$ increases, the variance in the fitted $\hat{\theta}$ decreases. We won't carry
out the full formal argument, but it suffices to make one observation that is
immediate from \cref{eq:theta_hat}: \textit{the variance of $\hat{\theta}$ is proportional to the eigenvalues of
$(X^\top X + \lambda I)^{-1}$}. To see this, observe that the eigenvalues of an inverse are just
the inverse of the eigenvalues:
\begin{equation}
    \operatorname{eig}\left( (X^\top X + \lambda I)^{-1} \right) = \left\{ \frac{1}{\sigma^2_1 + \lambda}, \ldots, \frac{1}{\sigma^2_d} + \lambda \right\}
\end{equation}

Now, condition on the points we draw, namely $X$. Then, recall that
randomness is in the label noise (recall the linear regression model $y \sim X\theta^* + \mathcal N(0,\tau^2 I) = \mathcal N(X\theta^* ,\tau^2 I)$).

Recall a fact about the multivariate normal distribution:
\begin{equation}
    \text{if } y \sim \mathcal \mathcal N(\mu,\Sigma) \text{ then } Ay \sim \mathcal N(A\mu,A\Sigma A^\top)
\end{equation}
Using linearity, we can verify that the expectation of $\hat{\theta}$ is:
\begin{align}
    \mathbb E[\hat{\theta}] &= \mathbb E\left[(X^\top X + \lambda I)^{-1} X^\top y\right]\\
        &= \mathbb E\left[(X^\top X + \lambda I)^{-1} X^\top (X\theta^* + \mathcal N(0,\tau^2 I))\right]\\ % TYPO: removed comma
        &= \mathbb E\left[(X^\top X + \lambda I)^{-1} X^\top (X\theta^* )\right]\\
        &= (X^\top X + \lambda I)^{-1} (X^\top X)\theta^* \tag{essentially a ``shrunk'' $\theta^*$}    
\end{align}
The last line above suggests that the more regularization we add (larger the $\lambda$),
the more the estimated $\hat{\theta}$ will be shrunk towards $0$. In other words,
regularization adds bias (towards zero in this case). Though we paid the cost of higher
bias, we gain by reducing the variance of
$\hat{\theta}$. To see this bias-variance tradeoff
concretely, observe the covariance matrix of $\hat{\theta}$:
\begin{align}
    C &:= \operatorname{Cov}[\hat{\theta}]\\
        &= \left( (X^\top X + \lambda I)^{-1} X^\top \right) (\tau^2 I) \left( X(X^\top X + \lambda I)^{-1} \right)\\
\text{and}&\nonumber\\
\operatorname{eig}(C) &= \left\{ \frac{\tau^2 \sigma^2_1}{(\sigma^2_1 + \lambda)^2}, \ldots, \frac{\tau^2 \sigma^2_d}{(\sigma^2_d + \lambda)^2}\right\}
\end{align}
Notice that the entire spectrum of the covariance is a decreasing function of $\lambda$.
By decomposing in the eigenvalue basis, we can see that actually $\mathbb E\left[\lVert \hat{\theta} - \theta^* \rVert^2 \right]$ is a decreasing function of $\lambda$, as desired.

\paragraph{Gradient descent} We show that you can initialize gradient descent in a way
that effectively regularizes undetermined least squares---even with no
regularization penalty ($\lambda = 0$). Our first observation is that any point $x \in \mathbb R^d$ can be
decomposed into two orthogonal components $x_0 ,x_1$ such that:
\begin{equation}
    x = x_0 + x_1 \text{ and } x_0 \in \operatorname{Null}(X) \text{ and } x_1 \in \operatorname{Range}(X^\top)    
\end{equation}
Recall that $\operatorname{Null}(X)$ and $\operatorname{Range}(X^\top)$ are orthogonal subspaces by the
fundamental theory of linear algebra. We write $P_0$ for the projection on the null and $P_1$
for the projection on the range, then $x_0 = P_0 (x)$ and $x_1 = P_1 (x)$.

If one initializes at a point $\theta$ then, we observe that the gradient is orthogonal
to the null space. That is, if $g(\theta) = X^\top (X\theta-y)$ then $g^\top P_0 (v) = 0$ for any $v \in \mathbb R^d$.
But, then:
\begin{equation}
    P_0 (\theta^{(t+1)}) = P_0 (\theta^t - \alpha g(\theta^{(t)})) = P_0 (\theta^t) - \alpha P_0 g(\theta^{(t)}) = P_0 (\theta^{(t)})
\end{equation}
That is, no learning happens in the null. Whatever portion is in the null that
we initialize stays there throughout execution.

A key property of the Moore-Penrose pseudoinverse, is that if $\hat{\theta} = (X^\top X) + X^\top y$
then $P_0(\hat{\theta}) = 0$. Hence, the gradient descent solution initialized at $\theta_0$ can be
written $\hat{\theta} + P_0 (\theta_0 )$. Two immediate observations:
\begin{itemize}
    \item Using the Moore-Penrose inverse acts as regularization, because it selects
    the solution $\hat{\theta}$.
    \item So does gradient descent---provided that we initialize at $\theta_0 = 0$. This
    is particularly interesting, as many modern machine learning techniques
    operate in these underdetermined regimes.
\end{itemize}

We've argued that there are many ways to find equivalent solutions, and that
this allows us to understand the effect on the model fitting procedure as
regularization. Thus, there are many ways to find that equivalent solution. Many
modern methods of machine learning including dropout and data augmentation
are not penalty, but their effect is understood as regularization. One contrast
with the above methods is that they often depend on some property of the data
or for how much they effectively regularization. In some sense, they adapt to
the data. A final comment is that in the same sense above, adding more data
regularizes the model as well!



\vspace{1cm}
\inlinechapter{Bias-variance and error analysis}
\marginnote{From CS229 Fall 2017, Yoann Le Calonnec, Stanford University.}
\section{The bias-variance tradeoff}
Assume you are given a well fitted machine learning model $\hat{f}$ that you want to apply on
some test dataset. For instance, the model could be a linear regression whose parameters
were computed using some training set different from your test set. For each point $x$ in your
test set, you want to predict the associated target $y \in \mathbb R$, and compute the mean squared
error (MSE):
\begin{equation}
    \mathbb E_{(x,y)\sim\text{test set}} \left[|\hat{f}(x) - y|^2\right]    
\end{equation}
You now realize that this MSE is too high, and try to find an explanation to this result:
\begin{itemize}
    \item \textit{Overfitting}: the model is too closely related to the examples in the training set and
    doesn't generalize well to other examples.
    \item \textit{Underfitting}: the model didn't gather enough information from the training set, and
    doesn't capture the link between the features $x$ and the target $y$.
    \item The data is simply noisy, that is the model is neither overfitting or underfitting, and
    the high MSE is simply due to the amount of noise in the dataset.
\end{itemize}
Our intuition can be formalized by the \textbf{bias-variance tradeoff}.

Assume that the points in your training/test set are all taken from a similar distribution,
with
\begin{equation}
    y_i = f(x_i) + \epsilon_i, \quad \text{where the noise $\epsilon_i$ satisfies } \quad \mathbb E(\epsilon_i) = 0, \operatorname{Var}(\epsilon_i ) = \sigma^2    
\end{equation}
and your goal is to compute $f$. By looking at your training set, you obtain an estimate $\hat{f}$.
Now use this estimate with your test set, meaning that for each example $j$ in the test set,
your prediction for $y_j = f(x_j) + \epsilon_j$ is $\hat{f}(x_j)$. Here, $x_j$ is a fixed real number (or vector if the
feature space is multi-dimensional) thus $f(x_j)$ is fixed, and $\epsilon_j$ is a real random variable with
mean $0$ and variance $\sigma^2$. The crucial observation is that $\hat{f}(x_j)$ is random since it depends on
the values $\epsilon_i$ from the training set. That's why talking about the bias $\mathbb E[ \hat{f}(x) - f(x)]$ and
the variance of $\hat{f}$ makes sense.

We can now compute our MSE on the test set by computing the following expectation
with respect to the possible training sets (since $\hat{f}$ is a random variable function of the choice
of the traning set):
\begin{align}
\text{test MSE} &= \mathbb E\left[(y - \hat{f}(x))^2 \right]\\
    &= \mathbb E\left[((\epsilon + f(x) - \hat{f}(x))^2 \right]\\
    &= \mathbb E[\epsilon^2] + \mathbb E\left[(f(x) - \hat{f}(x))^2 \right]\\
    &= \sigma^2 + \left( \mathbb E[f(x) - \hat{f}(x)] \right)^2 + \operatorname{Var}\left(f(x) - \hat{f}(x)\right)\\
    &= \sigma^2 + \left(\operatorname{Bias}\; \hat{f}(x)\right)^2 + \operatorname{Var}\left( \hat{f}(x) \right)
\end{align}

There is nothing we can do about the first term $\sigma^2$ as we can not predict the noise $\epsilon$ by
definition. The bias term is due to underfitting, meaning that on average, $\hat{f}$ does not predict
$f$. The last term is closely related to overfitting, the prediction $\hat{f}$ is too close from the values
$y$ train and varies a lot with the choice of our training set.

To sum up, we can understand our MSE as follows:
\begin{align*}
    \text{High Bias} &\;\longleftrightarrow\; \text{Underfitting}\\
    \text{High Variance} &\;\longleftrightarrow\; \text{Overfitting}\\
    \text{Large $\sigma^2$} &\;\longleftrightarrow\; \text{Noisy data}
\end{align*}

Hence, when analyzing the performance of a machine learning algorithm, we must always
ask ourselves how to reduce the bias without increasing the variance, and respectively how to
reduce the variance without increasing the bias. Most of the time, reducing one will increase
the other, and there is a tradeoff between bias and variance.

\section{Error analysis}
Even though understanding whether our poor test error is due to high bias or high variance
is important, knowing which parts of the machine learning algorithm lead to this error or
score is crucial. Consider the machine learning pipeline on \cref{fig:pipeline}. % TODO: figure from notes
The algorithms is divided into several steps:
\begin{enumerate}
    \item The inputs are taken from a camera image
    \item Preprocessing to remove the background on the image. For instance, if the image are
    taken from a security camera, the background is always the same, and we could remove
    it easily by keeping the pixels that changed on the image.
    \item Detect the position of the face.
    \item Detect the eyes - Detect the nose - Detect the mouth
    \item Final logistic regression step to predict the label
\end{enumerate}

% TODO.
% Figure 1: Face recognition pipeline

If you biuld a complicated system like this one, you might want to figure out how much
error is attributable to each of the components, how good is each of these green boxes.
Indeed, if one of these boxes is really problematic, you might want to spend more time
trying to improve the performance of that one green box. How do you decide what part to
focus on?

One thing we can do is plug in the ground-truth for each component, and see how
accuracy changes. Let's say the overall accuracy of the system is $85\%$ (pretty bad). You
can now take your development set and manually give it the perfect background removal,
that is, instead of using your background removal algorithm, manually specify the perfect
background removal yourself (using photoshop for instance), and look at how much that
affect the performance of the overall system.

Now let's say the accuracy only improves by $0.1\%$. This gives us an upperbound, that
is even if we worked for years on background removal, it wouldn't help our system by more
than $0.1\%$.

Now let's give the pipeline the perfect face detection by specifying the position of the
face manually, see how much we improve the performance, and so on.
The results are specified in the \cref{tab:accuracy}.

\begin{table}[!h]
  \centering
  \caption{
    \label{tab:accuracy} Accuracy when providing the system with the perfect component.
  }
  \begin{tabular}{cc}
    \toprule
    \textbf{Component} & \textbf{Accuracy}\\
    \midrule
    Overall system & $85\%$ \\
    Preprocess (remove background) & $85.1\%$ \\
    Face detection & $91\%$ \\
    Eyes segmentation & $95\%$ \\
    Nose segmentation & $96\%$ \\
    Mouth segmentation & $97\%$ \\
    Logistic regression & $100\%$ \\
    \bottomrule
  \end{tabular}
\end{table}

Looking at the table, we know that working on the background removal won't help much.
It also tells us where the biggest jumps are. We notice that having an accurate face detection
mechanism really improves the performance, and similarly, the eyes really help making the
prediction more accurate.

Error analysis is also useful when publishing a paper, since it's a convenient way to
analyze the error of an algorithm and explain which parts should be improved.

\section{Ablative analysis}
While error analysis tries to explain the difference between current performance and perfect
performance, ablative analysis tries to explain the difference between some baseline (much
poorer) performance and current performance.

For instance, suppose you have built a good anti-spam classifier by adding lots of clever
features to logistic regression
\begin{itemize}
    \item Spelling correction
    \item Sender host features
    \item Email header features
    \item Email text parser features
    \item Javascript parser
    \item Features from embedded images
\end{itemize}
and your question is: How much did each of these components really help?

In this example, let's say that simple logistic regression without any clever features gets
$94\%$ performance, but when adding these clever features, we get $99.9\%$ performance. In
abaltive analysis, what we do is start from the current level of performance $99.9\%$, and
slowly take away all of these features to see how it affects performance. The results are
provided in \cref{tab:ablation}.

\begin{table}[!h]
  \centering
  \caption{
    \label{tab:ablation} Accuracy when removing feature from logistic regression.
  }
  \begin{tabular}{cc}
    \toprule
    \textbf{Component} & \textbf{Accuracy}\\
    \midrule
    Overall system & $99.9\%$\\
    Spelling correction & $99.0\%$\\
    Sender host features & $98.9\%$\\
    Email header features & $98.9\%$\\
    Email text parser features & $95\%$\\
    Javascript parser & $94.5\%$\\
    Features from images & $94.0\%$\\
    \bottomrule
  \end{tabular}
\end{table}

When presenting the results in a paper, ablative analysis really helps analyzing the features
that helped decreasing the misclassification rate. Instead of simply giving the loss/error
rate of the algorithm, we can provide evidence that some specific features are actually more
important than others.

\subsection{Analyze your mistakes}
Assume you are given a dataset with pictures of animals, and your goal is to identify pictures
of cats that you would eventually send to the members of a community of cat lovers. You
notice that there are many pictures of dogs in the original dataset, and wonders whether
you should build a special algorithm to identify the pictures of dogs and avoid sending dogs
pictures to cat lovers or not.

One thing you can do is take a 100 examples from your development set that are
misclassified, and count up how many of these 100 mistakes are dogs. If $5\%$ of them are dogs,
then even if you come up with a solution to identidy your dogs, your error would only go
down by $5\%$, that is your accuracy would go up from $90\%$ to $90.5\%$. However, if 50 of these
100 errors are dogs, then you could improve your accuracy to reach $95\%$.

By analyzing your mistakes, you can focus on what's really important. If you notice that
80 out of your 100 mistakes are blurry images, then work hard on classifying correctly these
blurry images. If you notice that 70 out of the 100 errors are great cats, then focus on this
specific task of identifying great cats.

In brief, do not waste your time improving parts of your algorithm that won't really help
decreasing your error rate, and focus on what really matters.
